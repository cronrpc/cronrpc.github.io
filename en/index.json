[{"content":"Worley Noise Worley Noise1 (also known as Voronoi noise or Cellular noise) was introduced by Steven Worley2 in 1996 as a type of procedural noise function.\nIts appearance resembles biological cells, which is why the author originally called it “Cellular Texture.”\nThe algorithm for generating Worley Noise works as follows: the image is divided into a grid of cells, and each cell contains a single feature point. The value of any given pixel is the minimum distance from that pixel to the feature points in the surrounding cells.\nSince each cell contains only one feature point, each pixel only needs to check the 9 neighboring cells, making it a comparison of just 9 points in total.\nHere are two examples of its use in games3: a cracked desert floor and an exaggerated billowing smoke column.\nShadertoy For this implementation, I mainly learned from Suboptimal Engineer4 and The Book of Shaders5, while writing the actual code on Shadertoy6.\nAdditionally, iquilezles7 has a wealth of useful references on graphics programming.\nGrid Code Open Shadertoy and start with the following code:\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec3 col = vec3(0.0); fragColor = vec4(col,1.0); } This simply displays a black screen.\nNext, transform the coordinates into square grid coordinates, where each cell spans the range $[-0.5, +0.5]$.\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; vec3 col = vec3(coord, 0); fragColor = vec4(col,1.0); } Now color each pixel based on its distance from the edges of the grid cell.\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; float distanceGrid = 2.0 * max(abs(coord.x), abs(coord.y)); vec3 col = vec3(distanceGrid); fragColor = vec4(col,1.0); } Finally, only highlight the grid boundaries and tint them red.\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; float distanceGrid = smoothstep(0.9, 1.0, 2.0 * max(abs(coord.x), abs(coord.y))); vec3 girdColor = vec3(1.0, 0, 0) * vec3(distanceGrid); vec3 col = girdColor; fragColor = vec4(col,1.0); } Cell Code Next, we need to do two things:\nUse a pseudo-random function to determine the position of the feature point in each cell. For each pixel, compute the distance to feature points in the 9 neighboring cells and take the minimum. First, render the feature points, with one point per cell.\nvec2 random2( vec2 p ) { // add 0.5 to avoid vec2(0, 0) return (0, 0) return fract(sin(vec2(dot(p + 0.5, vec2(213.1,322.2)),dot(p + 0.5, vec2(513.1,312.2))))*51312.1234); } void mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; // Gird Color float distanceGrid = smoothstep(0.9, 1.0, 2.0 * max(abs(coord.x), abs(coord.y))); vec3 girdColor = vec3(1.0, 0, 0) * vec3(distanceGrid); // Cell Color float distancePoint = 1.0; for (int i=-1; i\u0026lt;=1; i++) { for (int j=-1; j\u0026lt;=1; j++) { vec2 near = vec2(float(i), float(j)); vec2 point = near + 0.5 * sin(iTime + 6.2831 * random2(gird + near)); float currentDistance = length(coord - point); distancePoint = min(currentDistance, distancePoint); } } vec3 pointColor = vec3(smoothstep(0.90, 1.0, 1.0 - distancePoint)); vec3 col = girdColor + pointColor; fragColor = vec4(col,1.0); } Once we have the distance, we can simply map it to a color.\nvec2 random2( vec2 p ) { // add 0.5 to avoid vec2(0, 0) return (0, 0) return fract(sin(vec2(dot(p + 0.5, vec2(213.1,322.2)),dot(p + 0.5, vec2(513.1,312.2))))*51312.1234); } void mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; // Gird Color float distanceGrid = smoothstep(0.95, 1.0, 2.0 * max(abs(coord.x), abs(coord.y))); vec3 girdColor = vec3(1.0, 0, 0) * vec3(distanceGrid); // Cell Color float distancePoint = 1.0; for (int i=-1; i\u0026lt;=1; i++) { for (int j=-1; j\u0026lt;=1; j++) { vec2 near = vec2(float(i), float(j)); vec2 point = near + 0.5 * sin(iTime + 6.2831 * random2(gird + near)); float currentDistance = length(coord - point); distancePoint = min(currentDistance, distancePoint); } } vec3 pointColor = vec3(smoothstep(0.95, 1.0, 1.0 - distancePoint)); vec3 distanceColor = vec3(smoothstep(0.2, 2.0, 1.7 - distancePoint)); vec3 col = girdColor + pointColor + distanceColor; fragColor = vec4(col,1.0); } Finally, if we remove the helper grid and the feature points, we get the actual Worley Noise image.\nPalettes Lastly, we can add some colors through gradient interpolation8.\nvec3 palette( in float t) { vec3 a = vec3(0.5, 0.5, 0.5); vec3 b = vec3(0.5, 0.5, 0.5); vec3 c = vec3(1.0, 1.0, 1.0); vec3 d = vec3(0.0, 0.1, 0.2); return a + b * cos( 6.283185 * ( c * t + d ) ); } vec2 random2( vec2 p ) { // add 0.5 to avoid vec2(0, 0) return (0, 0) return fract(sin(vec2(dot(p + 0.5, vec2(213.1,322.2)),dot(p + 0.5, vec2(513.1,312.2))))*51312.1234); } void mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = fragCoord/iResolution.y; uv = uv * 4.0; vec2 gird = floor(uv); vec2 coord = fract(uv) - 0.5; // Gird Color float distanceGrid = smoothstep(0.95, 1.0, 2.0 * max(abs(coord.x), abs(coord.y))); vec3 girdColor = vec3(1.0, 0, 0) * vec3(distanceGrid); // Cell Color float distancePoint = 1.0; for (int i=-1; i\u0026lt;=1; i++) { for (int j=-1; j\u0026lt;=1; j++) { vec2 near = vec2(float(i), float(j)); vec2 point = near + 0.5 * sin(iTime + 6.2831 * random2(gird + near)); float currentDistance = length(coord - point); distancePoint = min(currentDistance, distancePoint); } } vec3 pointColor = vec3(smoothstep(0.95, 1.0, 1.0 - distancePoint)); vec3 distanceColor = vec3(palette(smoothstep(0.2, 2.0, 1.7 - distancePoint))); vec3 col = distanceColor; col += girdColor + pointColor; fragColor = vec4(col,1.0); } wikipedia, Worley noise, https://en.wikipedia.org/wiki/Worley_noise\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSteven Worley, (1996), A Cellular Texture Basis Function, https://cedric.cnam.fr/~cubaud/PROCEDURAL/worley.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPor Ryan Brucks, (2016) , Getting the Most Out of Noise in UE4, https://www.unrealengine.com/es-ES/tech-blog/getting-the-most-out-of-noise-in-ue4\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSuboptimal Engineer, (2023), What is Voronoi Noise? , https://www.youtube.com/watch?v=vcfIJ5Uu6Qw\u0026ab_channel=SuboptimalEngineer\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPatricio Gonzalez Vivo and Jen Lowe, The Book of Shaders, https://thebookofshaders.com/12/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShadertoy, https://www.shadertoy.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\niquilezles, float small and random, https://iquilezles.org/articles/sfrand/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\niquilezles, palettes - 1999, https://iquilezles.org/articles/palettes/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://cronrpc.github.io/en/posts/worley-noise-generator/","summary":"\u003ch2 id=\"worley-noise\"\u003eWorley Noise\u003c/h2\u003e\n\u003cp\u003eWorley Noise\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e (also known as Voronoi noise or Cellular noise) was introduced by Steven Worley\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 1996 as a type of procedural noise function.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"worley noise\" loading=\"lazy\" src=\"/en/posts/worley-noise-generator/worley-noise.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIts appearance resembles biological cells, which is why the author originally called it “Cellular Texture.”\u003c/p\u003e\n\u003cp\u003eThe algorithm for generating Worley Noise works as follows: the image is divided into a grid of cells, and each cell contains a single feature point. The value of any given pixel is the minimum distance from that pixel to the feature points in the surrounding cells.\u003c/p\u003e","title":"Worley Noise / Voronoi Noise Generator"},{"content":"Introduction to SDF Signed Distance Field (SDF) Signed Distance Field (SDF)1 is a mathematical function or data structure used to represent shapes. In 2D or 3D space, it assigns each point a signed distance value, which represents the distance to the nearest surface (or boundary), with the sign distinguishing between inside and outside:\nPositive value: The point is outside the shape, and the value indicates the nearest distance to the surface. Negative value: The point is inside the shape, and the absolute value indicates the nearest distance to the surface. Zero: The point lies exactly on the surface of the shape. Formally, for a point $x$ in space, an SDF can be defined as:\n$$ f(x) = \\begin{cases} d(x, \\partial\\Omega), \u0026amp; \\text{if } x \\in \\Omega \\\\ -d(x, \\partial\\Omega), \u0026amp; \\text{if } x \\notin \\Omega \\\\ 0, \u0026amp; \\text{if } x \\in \\partial\\Omega \\end{cases} $$\nwhere $\\Omega$ denotes the interior of the object, $\\partial\\Omega$ its boundary, and $d(x, \\partial\\Omega)$ the minimum distance from $x$ to the boundary.\nGitHub Repository If you want to check out the final code that generates SDF images, you can find it in the GitHub repo2: cronrpc/Signed-Distance-Field-2D-Generator\nApplications The definition above may sound abstract, so let’s look at some practical applications:\nFont rendering (Valve’s SDF font technology3; Unity’s TMP fonts use SDF as well). Surface intersection in ray tracing\nStylized shadows, clouds, and similar effects\nIn general, any scenario involving continuous transitions can potentially leverage SDF techniques.\n2D SDF Generation Algorithms For a grayscale image where white represents the object and black represents the background, how do we generate an SDF image?\nBrute-Force Evaluation The simplest approach is brute force:\nIdentify all boundary pixels (a white pixel with at least one black neighbor). For every pixel, compute the nearest distance to the boundary set. Assign a positive or negative sign based on whether the pixel itself is inside (white) or outside (black). If the total number of pixels is $n$, the complexity is $O(n^2)$.\n# generator_sdf.py import sys import math from PIL import Image import numpy as np def generate_sdf(input_path, output_path): # Read grayscale image (0–255) img = Image.open(input_path).convert(\u0026#34;L\u0026#34;) w, h = img.size pixels = np.array(img) # Step 1: Find boundary pixels boundary_points = [] for y in range(h): for x in range(w): if pixels[y, x] \u0026gt; 127: neighbors = [ (nx, ny) for nx in (x - 1, x, x + 1) for ny in (y - 1, y, y + 1) if 0 \u0026lt;= nx \u0026lt; w and 0 \u0026lt;= ny \u0026lt; h and not (nx == x and ny == y) ] for nx, ny in neighbors: if pixels[ny, nx] \u0026lt;= 127: boundary_points.append((x, y)) break # Step 2 \u0026amp; 3: Compute SDF sdf = np.zeros((h, w), dtype=np.float32) for y in range(h): for x in range(w): min_dist = float(\u0026#34;inf\u0026#34;) for bx, by in boundary_points: dist = math.sqrt((x - bx) ** 2 + (y - by) ** 2) if dist \u0026lt; min_dist: min_dist = dist # Negative for background if pixels[y, x] \u0026lt;= 127: min_dist = -min_dist sdf[y, x] = min_dist # Normalize to 0–255 max_dist = np.max(np.abs(sdf)) sdf_normalized = ((sdf / max_dist) + 1) * 127.5 sdf_img = Image.fromarray(np.clip(sdf_normalized, 0, 255).astype(np.uint8)) sdf_img.save(output_path) print(f\u0026#34;SDF saved to {output_path}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) != 3: print(\u0026#34;Example: python generator_sdf.py test.png test_sdf.png\u0026#34;) sys.exit(1) generate_sdf(sys.argv[1], sys.argv[2]) Generated result:\nBy adjusting the display threshold in Photoshop, we can observe the SDF transitions:\nSimilarly, for a square, its SDF exhibits right-angle contours inside and circular contours outside:\n8-Neighborhood Euclidean Approximation A metric space 4 is an abstract mathematical framework that defines the notion of “distance.” It consists of a set $X$ and a distance function\n$$ d : X \\times X \\to \\mathbb{R} $$\nwhich must satisfy the following four conditions:\nNon-negativity: Distance is always non-negative. Identity: Distance is zero if and only if the two points are identical. Symmetry: The distance from $x$ to $y$ is the same as from $y$ to $x$. Triangle inequality: Taking a detour should never be shorter than going directly. $$ d(x, z) \\le d(x, y) + d(y, z) $$\nFor example, in the continuous 2D plane, the Euclidean distance\n$$ d(P, Q) = \\sqrt{(x_P - x_Q)^2 + (y_P - y_Q)^2} $$\nis a typical metric.\nIn the figure below, the blue, red, and yellow lines all represent the same length, while the green line is the shorter Euclidean distance.\nHowever, in digital image processing or raster maps, our points are discrete pixels. While distances can be computed using the Euclidean formula, doing so requires many square root operations, which are computationally expensive.\nTo improve efficiency, we often use the 8-neighborhood Euclidean approximation:\n8-neighborhood: Each pixel is considered adjacent to the 8 nearest pixels — horizontally, vertically, and diagonally. The approximate Euclidean distance is then defined as: $$ d_{8}(p, q) \\approx \\max(\\Delta x, \\Delta y) + (\\sqrt{2} - 1) \\cdot \\min(\\Delta x, \\Delta y) $$\nwhere $\\Delta x = |x_p - x_q|$，$\\Delta y = |y_p - y_q|$。\nFor example, in the figure below, the red path represents the 8-neighborhood Euclidean approximation, while the blue line is the true Euclidean distance.\nThis method avoids costly square root operations, and within the 8-neighborhood its error compared to the true Euclidean distance is small. Therefore, it is widely used in pathfinding (A*, Dijkstra), distance transforms, and morphological image processing.\n8SSEDT This algorithm is based on Lisapple’s 8SSEDT5.\nIn the previous section, we introduced the 8-neighborhood Euclidean approximation. In fact, for each node in the grid, its distance can be expressed recursively as:\n$$ f(x, y) = \\min_{\\substack{dx, dy \\in {-1,0,1} \\ (dx, dy) \\neq (0,0)}} \\Big( f(x+dx, y+dy) + d(dx, dy) \\Big) $$\nwhere $d(dx, dy)$ is the cost of moving from neighbor $(x+dx, y+dy)$ to $(x, y)$, usually defined as:\n$$ d(dx, dy) = \\begin{cases} 1, \u0026amp; \\text{where } |dx| + |dy| = 1 \\quad (\\text{horizontal or vertical neighbor})\\\\ \\sqrt{2}, \u0026amp; \\text{where } |dx| + |dy| = 2 \\quad (\\text{diagonal neighbor}) \\end{cases} $$\nHere, $dx$ and $dy$ can each be $-1, 0, 1$, but not both zero, since that would not be a neighbor.\nIf we label the neighbors:\n[#1][#2][#3] [#4][ x][#5] [#6][#7][#8] The path between any two points $x$ and $y$ must be composed of one of the following four combinations:\n1,2,4 2,3,5 4,6,7 5,7,8 Thus, propagating from the top-left to the bottom-right covers the $1,2,4$ case, meaning that at most four full passes over the grid are needed to compute all values.\nAn optimization, inspired by Signed Distance Fields6, chooses the following four traversal paths instead:\n- - - \u0026gt; | [?][?][?] | [?][x][ ] v [ ][ ][ ] \u0026lt; - - - | [ ][ ][ ] | [ ][x][?] v [ ][ ][ ] \u0026lt; - - - ^ [ ][ ][ ] | [ ][x][?] | [?][?][?] - - - \u0026gt; ^ [ ][ ][ ] | [?][x][ ] | [ ][ ][ ] Here is the Python implementation:\nStrictly speaking, the following code computes the true Euclidean distance, not the 8-neighborhood approximation. It only borrows the idea of 8 possible directions of movement. import sys import os import numpy as np from PIL import Image INF = 9999 def dist_sq(dx, dy): return dx*dx + dy*dy def compare(grid, p, x, y, ox, oy, width, height): nx = x + ox ny = y + oy if 0 \u0026lt;= nx \u0026lt; width and 0 \u0026lt;= ny \u0026lt; height: other_dx, other_dy = grid[ny, nx] else: other_dx, other_dy = INF, INF other_dx += ox other_dy += oy if dist_sq(other_dx, other_dy) \u0026lt; dist_sq(*p): p = (other_dx, other_dy) return p def generate_sdf(grid): height, width, _ = grid.shape # Pass 0 for y in range(height): for x in range(width): p = tuple(grid[y, x]) p = compare(grid, p, x, y, -1, 0, width, height) p = compare(grid, p, x, y, 0, -1, width, height) p = compare(grid, p, x, y, -1, -1, width, height) p = compare(grid, p, x, y, 1, -1, width, height) grid[y, x] = p for x in range(width-1, -1, -1): p = tuple(grid[y, x]) p = compare(grid, p, x, y, 1, 0, width, height) grid[y, x] = p # Pass 1 for y in range(height-1, -1, -1): for x in range(width-1, -1, -1): p = tuple(grid[y, x]) p = compare(grid, p, x, y, 1, 0, width, height) p = compare(grid, p, x, y, 0, 1, width, height) p = compare(grid, p, x, y, -1, 1, width, height) p = compare(grid, p, x, y, 1, 1, width, height) grid[y, x] = p for x in range(width): p = tuple(grid[y, x]) p = compare(grid, p, x, y, -1, 0, width, height) grid[y, x] = p def load_image_binary(path, threshold=128): im = Image.open(path).convert(\u0026#39;L\u0026#39;) arr = np.array(im, dtype=np.uint8) inside = arr \u0026lt; threshold return inside, im def save_signed_sdf_image(signed, out_path): max_dist = np.max(np.abs(signed)) if max_dist == 0: sdf_normalized = np.full_like(signed, 128.0) else: sdf_normalized = ((signed / max_dist) + 1.0) * 128.0 sdf_normalized = np.clip(sdf_normalized, 0, 255).astype(np.uint8) out = Image.fromarray(sdf_normalized, mode=\u0026#39;L\u0026#39;) out.save(out_path) def main(): if len(sys.argv) \u0026lt; 2: print(\u0026#34;Usage: python 8ssedt.py input.png\u0026#34;) sys.exit(1) in_path = sys.argv[1] if not os.path.exists(in_path): print(\u0026#34;File not found:\u0026#34;, in_path) sys.exit(1) inside, im = load_image_binary(in_path) h, w = inside.shape empty = (INF, INF) zero = (0, 0) # two grids: inside distances and outside distances grid1 = np.zeros((h, w, 2), dtype=int) grid2 = np.zeros((h, w, 2), dtype=int) for y in range(h): for x in range(w): if inside[y, x]: grid1[y, x] = zero grid2[y, x] = empty else: grid1[y, x] = empty grid2[y, x] = zero generate_sdf(grid1) generate_sdf(grid2) dist1 = np.sqrt(grid1[:, :, 0]**2 + grid1[:, :, 1]**2) dist2 = np.sqrt(grid2[:, :, 0]**2 + grid2[:, :, 1]**2) signed = dist1 - dist2 base, ext = os.path.splitext(in_path) out_path = base + \u0026#34;_8ssedt.png\u0026#34; save_signed_sdf_image(signed, out_path) print(\u0026#34;Saved:\u0026#34;, out_path) if __name__ == \u0026#34;__main__\u0026#34;: main() We can test it on this input image:\nAfter running the script, we get:\nVisualized in Photoshop with different thresholds:\nCorrect 8SSEDT As noted in 7, the issue with binary black-and-white images is that for pixels near the boundary, the true distance to the boundary should actually be only half a pixel.\nIgnoring this half-pixel offset leads to small inaccuracies near boundaries. The fix is to add only half the step size when the neighboring pixel is on the boundary.\nimport sys import os import math import numpy as np from PIL import Image FIX = True INF = 9999 def dist_sq(dx, dy): return dx*dx + dy*dy def compare(grid, p, x, y, ox, oy, width, height): nx = x + ox ny = y + oy if 0 \u0026lt;= nx \u0026lt; width and 0 \u0026lt;= ny \u0026lt; height: other_dx, other_dy = grid[ny, nx] else: other_dx, other_dy = INF, INF if FIX: if other_dx != 0 or other_dy != 0: ox *= 2 oy *= 2 other_dx += ox other_dy += oy if dist_sq(other_dx, other_dy) \u0026lt; dist_sq(*p): p = (other_dx, other_dy) return p def generate_sdf(grid): height, width, _ = grid.shape # Pass 0 for y in range(height): for x in range(width): p = tuple(grid[y, x]) p = compare(grid, p, x, y, -1, 0, width, height) p = compare(grid, p, x, y, 0, -1, width, height) p = compare(grid, p, x, y, -1, -1, width, height) p = compare(grid, p, x, y, 1, -1, width, height) grid[y, x] = p for x in range(width-1, -1, -1): p = tuple(grid[y, x]) p = compare(grid, p, x, y, 1, 0, width, height) grid[y, x] = p # Pass 1 for y in range(height-1, -1, -1): for x in range(width-1, -1, -1): p = tuple(grid[y, x]) p = compare(grid, p, x, y, 1, 0, width, height) p = compare(grid, p, x, y, 0, 1, width, height) p = compare(grid, p, x, y, -1, 1, width, height) p = compare(grid, p, x, y, 1, 1, width, height) grid[y, x] = p for x in range(width): p = tuple(grid[y, x]) p = compare(grid, p, x, y, -1, 0, width, height) grid[y, x] = p def load_image_binary(path, threshold=128): im = Image.open(path).convert(\u0026#39;L\u0026#39;) arr = np.array(im, dtype=np.uint8) inside = arr \u0026lt; threshold return inside, im def save_signed_sdf_image(signed, out_path): max_dist = np.max(np.abs(signed)) if max_dist == 0: sdf_normalized = np.full_like(signed, 128.0) else: sdf_normalized = ((signed / max_dist) + 1.0) * 128.0 sdf_normalized = np.clip(sdf_normalized, 0, 255).astype(np.uint8) out = Image.fromarray(sdf_normalized, mode=\u0026#39;L\u0026#39;) out.save(out_path) def main(): if len(sys.argv) \u0026lt; 2: print(\u0026#34;Usage: python 8ssedt.py input.png\u0026#34;) sys.exit(1) in_path = sys.argv[1] if not os.path.exists(in_path): print(\u0026#34;File not found:\u0026#34;, in_path) sys.exit(1) inside, im = load_image_binary(in_path) h, w = inside.shape empty = (INF, INF) zero = (0, 0) # two grids: inside distances and outside distances grid1 = np.zeros((h, w, 2), dtype=int) grid2 = np.zeros((h, w, 2), dtype=int) for y in range(h): for x in range(w): if inside[y, x]: grid1[y, x] = zero grid2[y, x] = empty else: grid1[y, x] = empty grid2[y, x] = zero generate_sdf(grid1) generate_sdf(grid2) dist1 = np.sqrt(grid1[:, :, 0]**2 + grid1[:, :, 1]**2) dist2 = np.sqrt(grid2[:, :, 0]**2 + grid2[:, :, 1]**2) if FIX: signed = 0.5 * (dist1 - dist2) else: signed = dist1 - dist2 base, ext = os.path.splitext(in_path) out_path = base + \u0026#34;_8ssedt_correct.png\u0026#34; save_signed_sdf_image(signed, out_path) print(\u0026#34;Saved:\u0026#34;, out_path) if __name__ == \u0026#34;__main__\u0026#34;: main() The results look nearly identical to the original:\nBut when visualized at different thresholds, the corrected version is slightly smoother near boundaries:\nCorrected:\nOriginal:\nWhy Still Use Euclidean Distance? Consider the approximation formula:\n$$ d_8(p,q) = \\max(\\Delta x,\\Delta y) + (\\sqrt{2} - 1) \\cdot \\min(\\Delta x,\\Delta y) $$\nTake two points: $(1,4)$ and $(3,3)$.\n计算 $d_8$\n$d_8(1,4) = 4 + (\\sqrt{2} - 1) \\cdot 1 \\approx 4 + 0.4142 = 4.4142$\n$d_8(3,3) = 3 + (\\sqrt{2} - 1) \\cdot 3 \\approx 3 + 1.2426 = 4.2426$\n⇒ $d_8(1,4) \u0026gt; d_8(3,3)$\n计算欧几里得距离\n$d_E(1,4) = \\sqrt{1^2 + 4^2} = \\sqrt{17} \\approx 4.1231$\n$d_E(3,3) = \\sqrt{3^2 + 3^2} = \\sqrt{18} \\approx 4.2426$\n⇒ $d_E(1,4) \u0026lt; d_E(3,3)$\nThis counterexample shows that $d_8$ does not always preserve the same ordering as Euclidean distance. Therefore, we only borrow the 8-neighborhood displacements, but still compute distances using the Euclidean metric.\nThe scipy.ndimage Library The previous Python implementation is slow due to nested loops, especially at resolutions like 2048×2048.\nscipy.ndimage provides a much faster alternative, as its image processing functions are implemented in C++.\nIn the following code, we use a fixed scale of 8 (explained later), and output 16-bit grayscale images instead of 8-bit.\nimport sys import os import numpy as np from PIL import Image from scipy import ndimage scale = 8 def load_image_binary(path, threshold=128): im = Image.open(path).convert(\u0026#39;L\u0026#39;) arr = np.array(im, dtype=np.uint8) inside = arr \u0026lt; threshold return inside def save_sdf_16bit(signed, out_path): signed = signed * scale + 32767.5 signed_uint16 = np.clip(signed, 0, 65535).astype(np.uint16) out = Image.fromarray(signed_uint16, mode=\u0026#39;I;16\u0026#39;) out.save(out_path) def fast_signed_sdf(mask): dist_inside = ndimage.distance_transform_edt(mask) dist_outside = ndimage.distance_transform_edt(~mask) return dist_outside - dist_inside def main(): if len(sys.argv) \u0026lt; 2: print(\u0026#34;Usage: python fast_sdf.py input1.png [input2.png ...]\u0026#34;) sys.exit(1) for in_path in sys.argv[1:]: if not os.path.exists(in_path): print(\u0026#34;File not found:\u0026#34;, in_path) continue inside = load_image_binary(in_path) signed = fast_signed_sdf(inside) base, _ = os.path.splitext(in_path) out_path = base + \u0026#34;_sdf16.png\u0026#34; save_sdf_16bit(signed, out_path) print(f\u0026#34;Saved SDF to {out_path}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 16bit 和 scale Earlier, we normalized distances, but if we want to combine multiple images, the distance units must be consistent (since interpolation between SDFs depends on this).\nFurthermore, with large images (e.g., 2048×2048), 8-bit depth (0–255) is insufficient to represent the full range of distances. To avoid truncation artifacts, we use 16-bit PNGs.\nFor a 2048×2048 image, the maximum possible distance is about $2048 \\cdot 1.414 \\approx 2896$. Since 16-bit integers can represent up to $\\pm32767$, we can safely choose a scale factor of 8 or 10.\nscale = 10 # Map to 0–65535, 32767.5 as the boundary unsigned = signed * scale + 32767.5 unsigned = np.clip(unsigned, 0, 65535).astype(np.uint16) Composition Algorithms How to combine SDFs depends on the desired effect.\nFor example, if we want a subtraction, can we simply subtract the distance fields?\nThis works at the midpoint, but at the edges the meaning becomes unclear:\n# Boolean operations def sdf_union(d1, d2): return np.minimum(d1, d2) # Union def sdf_intersection(d1, d2): return np.maximum(d1, d2) # Intersection def sdf_subtraction(d1, d2): return np.maximum(d1, -d2) # A subtract B These operations essentially define the effect when crossing the threshold boundary.\nInterpolation Approach In games, what we often want is this: given two images a and b, at maximum threshold we see a, at minimum threshold we see b.\nFor three images, the highest threshold shows a, the middle threshold shows b, and the lowest threshold shows c, and so on.\nA key requirement is that they must have a containment relationship: $c \\supset b \\supset a$. Otherwise, the transition cannot be well-defined.\nFor an arbitrary pixel in a, its distance values in both a and b should be positive (inside).\nFor a pixel in $b - a$, its distance in a will be negative ($x_a$), and in b positive ($x_b$). Thus, the final SDF value $x_{final}$ is determined by finding the threshold where interpolation between $x_a$ and $x_b$ crosses zero.\nFor example, if ${sdf}_a = -3$ and ${sdf}b = 3$, then at 8-bit gray, ${x}{final}$ should be 128 — meaning the pixel is lit at thresholds below 128.\nSo the core idea is: find the zero-crossing point.\nMonte Carlo Interpolation We can approximate the zero point via sampling, as in 8.\nNote that pure Python with nested loops is too slow for large images (\u0026gt;2048).\nimport sys import numpy as np from PIL import Image if len(sys.argv) != 3: print(\u0026#34;Usage: python compose2.py a.png b.png\u0026#34;) sys.exit(1) # Load a 16-bit grayscale image def load_16bit_gray(path): img = Image.open(path) arr = np.array(img, dtype=np.uint16) return arr sdf1 = load_16bit_gray(sys.argv[1]) sdf2 = load_16bit_gray(sys.argv[2]) # Ensure dimensions match if sdf1.shape != sdf2.shape: raise ValueError(\u0026#34;The two input images must have the same dimensions\u0026#34;) height, width = sdf1.shape output = np.zeros((height, width), dtype=np.uint16) THRESHOLD = 32768 # 16-bit midpoint (0.5 grayscale) MAX_VAL = 65535 STEPS = 16 for y in range(height): for x in range(width): t1 = sdf1[y, x] t2 = sdf2[y, x] if t1 \u0026lt; THRESHOLD and t2 \u0026lt; THRESHOLD: result = 0 elif t1 \u0026gt; THRESHOLD and t2 \u0026gt; THRESHOLD: result = MAX_VAL else: # Interpolate between the two images result = 0 for i in range(STEPS): weight = i / STEPS interp = (1 - weight) * t1 + weight * t2 result += 0 if interp \u0026lt; THRESHOLD else MAX_VAL result //= STEPS output[y, x] = np.clip(result, 0, MAX_VAL) # Save as 16-bit PNG out_img = Image.fromarray(output, mode=\u0026#39;I;16\u0026#39;) out_img.save(\u0026#34;output.png\u0026#34;) print(\u0026#34;Composition complete: output.png\u0026#34;) Final Algorithm This interpolation is highly parallel: each pixel is independent, and interpolating across images is also independent.\nUsing NumPy for vectorized operations achieves high performance (NumPy is backed by optimized C++).\nHere we sample 256 points across the grayscale range, and output an 8-bit grayscale image (16-bit is unnecessary for the final composite).\nimport sys import numpy as np from PIL import Image U8 = True if len(sys.argv) \u0026lt; 3: print(\u0026#34;Example: python compose.py img1.png img2.png [img3.png ...]\u0026#34;) sys.exit(1) def load_16bit_gray(path): img = Image.open(path) arr = np.array(img, dtype=np.uint16) return arr images = [load_16bit_gray(path) for path in sys.argv[1:]] arr = np.stack(images, axis=0) # shape: (N, H, W) if not np.all([img.shape == arr[0].shape for img in images]): raise ValueError(\u0026#34;All input images must have the same dimensions\u0026#34;) THRESHOLD = 32768 MAX_VAL = 65535 N, H, W = arr.shape # Global masks all_below = np.all(arr \u0026lt; THRESHOLD, axis=0) all_above = np.all(arr \u0026gt; THRESHOLD, axis=0) output = np.zeros((H, W), dtype=np.float64) output[all_below] = 0 output[all_above] = MAX_VAL # Mixed region mask mix_mask = ~(all_below | all_above) # Extract pixels in the mixed region mix_pixels = arr[:, mix_mask].astype(np.float64) # shape: (N, M) M = number of mixed pixels M = mix_pixels.shape[1] # 256 sampling weights samples = 256 weights = np.linspace(0, 1, samples, endpoint=False) # Which two images each sample point belongs to intervals = np.floor(weights * (N - 1)).astype(int) # shape: (samples,) local_w = (weights * (N - 1)) - intervals # shape: (samples,) # Batch interpolation interp_results = np.zeros((samples, M), dtype=np.float64) for k in range(samples): i1 = intervals[k] i2 = i1 + 1 val = (1 - local_w[k]) * mix_pixels[i1] + local_w[k] * mix_pixels[i2] interp_results[k] = (val \u0026gt;= THRESHOLD) * MAX_VAL # Average results res = np.mean(interp_results, axis=0) # Write results back output[mix_mask] = res if U8: # Convert to float, map to 0–255 output_float = output.astype(np.float32) output_scaled = output_float / 65535.0 * 255.0 output_uint8 = np.clip(np.floor(output_scaled + 0.5), 0, 255).astype(np.uint8) # Save 8-bit grayscale out_img = Image.fromarray(output_uint8, mode=\u0026#39;L\u0026#39;) out_img.save(\u0026#34;output8.png\u0026#34;) print(f\u0026#34;Composition complete: output8.png, merged {N} images\u0026#34;) else: output = np.clip(output, 0, MAX_VAL).astype(np.uint16) out_img = Image.fromarray(output, mode=\u0026#39;I;16\u0026#39;) out_img.save(\u0026#34;output16.png\u0026#34;) print(f\u0026#34;Composition complete: output16.png, merged {N} images\u0026#34;) Result:\nOriginal images:\nGenerated SDFs (scaled, so they appear grayish):\nComposited SDF:\nwikipedia, Signed distance function, https://en.wikipedia.org/wiki/Signed_distance_function\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ncronrpc, (2025), Signed Distance Field 2D Generator, https://github.com/cronrpc/Signed-Distance-Field-2D-Generator\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChris Green. Valve. (2007). Improved Alpha-Tested Magnification for Vector Textures and Special Effects, https://steamcdn-a.akamaihd.net/apps/valve/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMetric Space, https://en.wikipedia.org/wiki/Metric_space\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLisapple, (2017), 8SSEDT , GitHub, https://github.com/Lisapple/8SSEDT\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRichard Mitton, (2009), Signed Distance Fields, http://www.codersnotes.com/notes/signed-distance-fields/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nfarteryhr, Correct 8SSEDT, https://replit.com/@farteryhr/Correct8SSEDT#main.cpp\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n蛋白胨, Unity 卡通渲染 程序化天空盒, https://zhuanlan.zhihu.com/p/540692272\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://cronrpc.github.io/en/posts/signed-distance-field/","summary":"\u003ch2 id=\"introduction-to-sdf\"\u003eIntroduction to SDF\u003c/h2\u003e\n\u003ch3 id=\"signed-distance-field-sdf\"\u003eSigned Distance Field (SDF)\u003c/h3\u003e\n\u003cp\u003eSigned Distance Field (SDF)\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e is a mathematical function or data structure used to represent shapes.\nIn 2D or 3D space, it assigns each point a signed distance value, which represents the distance to the nearest surface (or boundary), with the sign distinguishing between inside and outside:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePositive value: The point is outside the shape, and the value indicates the nearest distance to the surface.\u003c/li\u003e\n\u003cli\u003eNegative value: The point is inside the shape, and the absolute value indicates the nearest distance to the surface.\u003c/li\u003e\n\u003cli\u003eZero: The point lies exactly on the surface of the shape.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFormally, for a point $x$ in space, an SDF can be defined as:\u003c/p\u003e","title":"Signed Distance Field"},{"content":"Introduction to Perlin Noise Perlin Noise was developed by Ken Perlin in 1983 for the film Tron as a smooth pseudo-random noise algorithm1. It can generate random patterns with natural-looking texture and is widely used in computer graphics to simulate natural phenomena such as clouds, terrain, fire, wood grain, and water flow2.\nUnlike plain white noise, Perlin Noise has spatial correlation: values at neighboring sample points vary smoothly without abrupt jumps. This smoothness makes the generated textures resemble the continuous variations found in nature.\nMinecraft, for example, uses Perlin Noise as a foundational algorithm to generate its effectively infinite world maps.\nTypes of Noise 1D Uniform White Noise 1D uniform white noise is a one-dimensional noise signal where values follow a uniform distribution.\nimport numpy as np import matplotlib.pyplot as plt def sample_random_points(num_points): # Sampler function: return num_points random numbers in the range [0, 1) return np.random.rand(num_points) def draw_random_strip_image_from_samples(rand_1d, width, height, filename, thickness=1): num_points = len(rand_1d) cols_per_point = width // num_points rand_pos = (rand_1d * (height - 1)).astype(int) img = np.zeros((height, width)) for i, row in enumerate(rand_pos): start_row = max(row - thickness, 0) end_row = min(row + thickness + 1, height) start_col = i * cols_per_point end_col = start_col + cols_per_point img[start_row:end_row, start_col:end_col] = 1 plt.imshow(img, cmap=\u0026#39;gray\u0026#39;, origin=\u0026#39;lower\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() plt.imsave(filename, img, cmap=\u0026#39;gray\u0026#39;) if __name__ == \u0026#34;__main__\u0026#34;: rand_noise = sample_random_points(512) draw_random_strip_image_from_samples(rand_noise, 1024, 512, \u0026#34;random_noise_1d.png\u0026#34;) For one-dimensional random noise, the generated points have no correlation between them.\nIf we use Perlin Noise instead, we get a continuous and smooth curve.\n2D Uniform White Noise If we randomly generate a noise image with a uniform distribution, the result is uniform white noise: each point has no relation to its neighbors.\nimport numpy as np import matplotlib.pyplot as plt # Generate a 512x512 random floating-point grayscale image with values in [0, 1) noise = np.random.rand(512, 512) plt.imshow(noise, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.imsave(\u0026#39;random_noise.png\u0026#39;, noise, cmap=\u0026#39;gray\u0026#39;) Completely random white noise:\nPerlin Noise, by contrast, has correlations between neighboring points — adjacent values are required to be close and smooth.\nGaussian White Noise import numpy as np import matplotlib.pyplot as plt # Generate 512x512 Gaussian white noise with mean 0.5 and std deviation 0.1 noise = np.random.normal(loc=0.5, scale=0.1, size=(512, 512)) # Clip values to [0, 1] to avoid display artifacts noise = np.clip(noise, 0, 1) plt.imshow(noise, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.imsave(\u0026#39;gaussian_white_noise_2d.png\u0026#39;, noise, cmap=\u0026#39;gray\u0026#39;) plt.show() Because it is not uniformly distributed, Gaussian noise often looks a bit softer than uniform white noise.\nThe Gaussian distribution is even more apparent in one dimension, where a clear central band concentrates most of the points.\n1D Perlin Noise To understand the fractal-related concepts in Perlin Noise, we first need to understand the ideas of fade functions, periodicity, and frequency multipliers (octaves)3.\nFade Given a series of known point values, to estimate values between them we can use linear interpolation to produce straight-line segments or cosine interpolation to produce smooth curves.\nCosine interpolation can be written as:\n$$ x = x1 + (x2-x1) * \\frac{1 - \\cos(\\pi \\alpha)}{2} $$\nThe goal of different interpolation methods is to produce a smooth transition rather than a sharp corner. Besides cosine, cubic, quintic, and other polynomials are commonly used.\nConverting linear interpolation into a smooth interpolation like this is called the Fade operation.\nIn the improved version of his noise algorithm, Perlin used the following Fade function:\n$$ t = 6 t^5 - 15 t^4 + 10 t^3 $$\nOctaves If you repeatedly double the sampling frequency, you obtain functions with halved periods. Because doubling frequency corresponds to musical octaves, these levels are called octaves.\nPerlin Noise is composed by mixing noises of different periods with certain weights.\nTypically, periods are doubled while weights are halved, which introduces a fractal-like concept.\n2D Perlin Noise Generator Algorithm Ken Perlin published an implementation of his improved Perlin Noise on his website4.\nPerlin Noise is a grid-based gradient noise function commonly used to generate natural-looking textures and terrain. The two-dimensional implementation proceeds as follows:\n1. Grid partitioning Divide the 2D space into a regular integer grid. Each grid corner (lattice point) is associated with a random gradient vector (often unit vectors uniformly distributed in direction).\n2. Compute the sample point\u0026rsquo;s local position Given a sample point $(x, y)$, determine its containing grid cell by computing the integer coordinates $(X, Y)$ of the cell\u0026rsquo;s lower-left corner:\n$$ X = \\lfloor x \\rfloor, \\quad Y = \\lfloor y \\rfloor $$\nCompute the sample point\u0026rsquo;s local coordinates relative to the lower-left corner:\n$$ x_{\\text{rel}} = x - X, \\quad y_{\\text{rel}} = y - Y $$\n3. Compute dot products between corner gradients and displacement vectors For the four corners of the cell:\n$$ (X, Y),\\ (X+1, Y),\\ (X, Y+1),\\ (X+1, Y+1) $$\nretrieve each corner\u0026rsquo;s gradient vector $\\vec{g}$ and compute the dot product between $\\vec{g}$ and the displacement vector $\\vec{d}$ from the corner to the sample point:\n$$ \\text{dot} = \\vec{g} \\cdot \\vec{d} $$\n4. Smooth interpolation Apply a smooth interpolation function (such as Perlin\u0026rsquo;s fade) along the $x$ and $y$ axes to bilinearly interpolate the four dot products:\n$$ u = \\text{fade}(x_{\\text{rel}}), \\quad v = \\text{fade}(y_{\\text{rel}}) $$\n5. Output noise value The interpolated result is the noise value at the sample point $(x,y)$, typically in the range approximately $[-1, 1]$.\nPython Code Below is an implementation of 2D Perlin Noise in Python.\nimport numpy as np import matplotlib.pyplot as plt permutation = np.array([ 151,160,137,91,90,15, 131,13,201,95,96,53,194,233,7,225,140,36,103,30,69,142,8,99,37,240,21,10,23, 190, 6,148,247,120,234,75,0,26,197,62,94,252,219,203,117,35,11,32,57,177,33, 88,237,149,56,87,174,20,125,136,171,168, 68,175,74,165,71,134,139,48,27,166, 77,146,158,231,83,111,229,122,60,211,133,230,220,105,92,41,55,46,245,40,244, 102,143,54, 65,25,63,161, 1,216,80,73,209,76,132,187,208, 89,18,169,200,196, 135,130,116,188,159,86,164,100,109,198,173,186, 3,64,52,217,226,250,124,123, 5,202,38,147,118,126,255,82,85,212,207,206,59,227,47,16,58,17,182,189,28,42, 223,183,170,213,119,248,152, 2,44,154,163, 70,221,153,101,155,167, 43,172,9, 129,22,39,253, 19,98,108,110,79,113,224,232,178,185, 112,104,218,246,97,228, 251,34,242,193,238,210,144,12,191,179,162,241, 81,51,145,235,249,14,239,107, 49,192,214, 31,181,199,106,157,184, 84,204,176,115,121,50,45,127, 4,150,254, 138,236,205,93,222,114,67,29,24,72,243,141,128,195,78,66,215,61,156,180 ], dtype=np.uint16) p = np.tile(permutation, 2) def grad(hash:int, x:float, y:float)-\u0026gt;float: grad_vectors = np.array([ [1, 1], [-1, 1], [1, -1], [-1, -1], [1, 0], [-1, 0], [0, 1], [0, -1] ]) # Normalize gradient vectors grad_vectors = grad_vectors / np.linalg.norm(grad_vectors, axis=1)[:, None] g = grad_vectors[hash % 8] return np.dot(g, [x, y]) def lerp(t: float, a: float, b: float)-\u0026gt;float: return a + t * (b - a) def fade(t: float)-\u0026gt;float: return t * t * t * (t * (t * 6 - 15) + 10) def get_noise(x:float, y:float): X = int(np.floor(x)) \u0026amp; 255 Y = int(np.floor(y)) \u0026amp; 255 x = x % 1 y = y % 1 u = fade(x) v = fade(y) ## Here we only need to ensure the hash values are unique x1 = p[p[X] + Y] x2 = p[p[X+1] + Y] x3 = p[p[X] + Y + 1] x4 = p[p[X+1] + Y + 1] ## x3 x4 ## x1 x2 ## The direction of each vector is from the integer lattice point towards the noise sample point return lerp(v, lerp(u, grad(x1, x, y), grad(x2, x - 1, y)), lerp(u, grad(x3, x, y - 1), grad(x4, x - 1, y - 1))) def get_perlin_noise(width=256, height=256, scale=8.0, octaves=1, persistence=0.5, lacunarity=2.0): noise_arr = np.zeros((height, width), dtype=np.float32) for i in range(height): for j in range(width): for k in range(octaves): x = (j / width) * scale * (lacunarity ** k) y = (i / height) * scale * (lacunarity ** k) noise_arr[i, j] += (persistence ** k) * get_noise(x, y) return noise_arr fig, axs = plt.subplots(2, 2, figsize=(10, 10)) for octave_count, ax in zip(range(1, 5), axs.flatten()): noise_arr = get_perlin_noise(width=256, height=256, scale=8.0, octaves=octave_count) # Optionally normalize to [0, 1] noise_arr = (noise_arr - noise_arr.min()) / (noise_arr.max() - noise_arr.min()) ax.imshow(noise_arr, cmap=\u0026#39;gray\u0026#39;) ax.set_title(f\u0026#39;octaves = {octave_count}\u0026#39;, fontsize=14) ax.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.savefig(\u0026#39;perlin_noise_octaves_1_to_4.png\u0026#39;) How to improve gradient values Using fixed gradient directions instead of gradients produced by a fully uniform random hash helps avoid directional bias that degrades the natural appearance5.\nIn the original approach, gradient directions were uniformly distributed over the sphere. However, a cube is not a sphere: projections along coordinate axes are shorter while diagonal directions are longer.\nThis asymmetry in direction can cause a sparse clustering effect: when several nearby gradients that are nearly axis-aligned happen to point the same way, those regions can show anomalously high values.\nIn his improved version, Perlin suggested choosing gradients like:\n$$ (1,1,0),(-1,1,0),(1,-1,0),(-1,-1,0), (1,0,1),(-1,0,1),(1,0,-1),(-1,0,-1), (0,1,1),(0,-1,1),(0,1,-1),(0,-1,-1) $$\nTo avoid the cost of dividing by 12, he expanded the set to 16 directions by adding $(1,1,0),(-1,1,0),(0,-1,1),(0,-1,-1)$.\nThis article follows that idea: it uses eight planar directions for gradients, and each gradient is normalized to unit length.\nFor one-dimensional Perlin Noise, you can omit normalization to increase variation. A 1D Perlin Noise can be seen as sampling a 2D Perlin Noise along a line through the lattice; lattice points off the line have zero weight and the result is equivalent to the 1D case. In that situation, the projection of a diagonal gradient onto the horizontal axis is not 1 but $\\frac{\\sqrt{2}}{2}$.\nImproving the Fade function Perlin5 proposed replacing the fade function with a smoother polynomial (the quintic above).\nThe reason is: if the second derivative is not smooth, when used to displace surfaces you can see obvious blocky artifacts.\nSo it\u0026rsquo;s best to follow these principles:\nAt 0 and 1, derivatives should be 0 so transitions are as smooth as possible. Higher-order derivatives can also be made zero. $ f(0) = 0 $ $ f(1) = 1 $ Examples / Applications Python Noise Library In practice, you rarely need to implement Perlin Noise from scratch — the Python noise library can generate Perlin Noise directly.\nimport numpy as np import matplotlib.pyplot as plt from noise import pnoise2 width, height = 512, 512 scale = 64.0 # Controls the \u0026#34;stretch\u0026#34; of the noise; larger values make changes occur more slowly octaves = 6 persistence = 0.5 lacunarity = 2.0 noise = np.zeros((height, width)) for y in range(height): for x in range(width): nx = x / scale ny = y / scale noise[y][x] = pnoise2(nx, ny, octaves=octaves, persistence=persistence, lacunarity=lacunarity, repeatx=1024, repeaty=1024, base=0) # Perlin noise is typically in approximately [-1, 1]; normalize to [0, 1] noise = (noise - noise.min()) / (noise.max() - noise.min()) plt.imshow(noise, cmap=\u0026#39;gray\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.imsave(\u0026#39;perlin_noise_2d_lib.png\u0026#39;, noise, cmap=\u0026#39;gray\u0026#39;) plt.show() Because it is optimized, generation is very fast:\nClouds Use Perlin Noise to simulate simple cloud textures.\nSimulate the dissipating edges of clouds. Simulate central highlights; above a threshold, values remain unchanged, and within a certain range apply the following operation: $$ \\text{Intensity} = \\max(\\text{Intensity} - 40, 0) $$\nimport numpy as np from PIL import Image # Read grayscale image, keep integer values in 0~255 img = Image.open(\u0026#39;perlin_noise_2d_lib.png\u0026#39;).convert(\u0026#39;L\u0026#39;) arr = np.array(img).astype(np.int32) # 0~255 height, width = arr.shape # Create an RGB image, pure blue background (R,G,B) = (0,0,255) rgb_img = np.zeros((height, width, 3), dtype=np.uint8) for y in range(height): for x in range(width): r, g, b = 0, 0, 255 cloud_intensity = arr[y, x] # 0~255 cloud_intensity = cloud_intensity - 30 if cloud_intensity \u0026lt; 100: cloud_intensity = max(cloud_intensity - 40, 0) # Linear blend: cloud_intensity represents white cloud strength (0~255) # Compute weight: cloud_intensity / 255 w = cloud_intensity / 255 r = int(r * (1 - w) + w * 255) g = int(g * (1 - w) + w * 255) b = int(b * (1 - w) + w * 255) rgb_img[y, x, 0] = r rgb_img[y, x, 1] = g rgb_img[y, x, 2] = b # Save the uint8 array directly im = Image.fromarray(rgb_img) im.save(\u0026#39;perlin_cloud.png\u0026#39;) Cloud result:\nUnity Noise Editor After installing the Terrain Tools package in Unity, you can find the noise generator under Window \u0026gt; Terrain \u0026gt; Edit Noise. It allows you to directly generate various types of noise, including Perlin Noise, and export them as textures.\nPerlin, K. (1985). An image synthesizer. ACM Siggraph Computer Graphics, 19(3), 287-296.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPerlin Noise wiki, https://en.wikipedia.org/wiki/Perlin_noise\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoger Eastman. (2019). CMSC425.01 Spring 2019 Lecture 20: Perlin noise I. University of Maryland. https://www.cs.umd.edu/class/spring2019/cmsc425/handouts/CMSC425Day20.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKen Perlin. (2022). Improved Noise reference implementation. https://mrl.cs.nyu.edu/~perlin/noise/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPerlin, K. (2002, July). Improving noise. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques (pp. 681-682).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://cronrpc.github.io/en/posts/perlin-noise-generator/","summary":"\u003ch2 id=\"introduction-to-perlin-noise\"\u003eIntroduction to Perlin Noise\u003c/h2\u003e\n\u003cp\u003ePerlin Noise was developed by Ken Perlin in 1983 for the film \u003cem\u003eTron\u003c/em\u003e as a smooth pseudo-random noise algorithm\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. It can generate random patterns with natural-looking texture and is widely used in computer graphics to simulate natural phenomena such as clouds, terrain, fire, wood grain, and water flow\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eUnlike plain white noise, Perlin Noise has spatial correlation: values at neighboring sample points vary smoothly without abrupt jumps. This smoothness makes the generated textures resemble the continuous variations found in nature.\u003c/p\u003e","title":"Perlin Noise Generator"},{"content":"This article focuses solely on the following topics:\nTransition from trigonometric series expansion to the Euler form of the Fourier transform What the Discrete Fourier Transform is What the 2D Fourier Transform is Explanation of the 2D Discrete Fourier Transform from the 2D Fourier Transform How to perform a 2D Fourier transform on an image Periodicity issues in the 2D Fourier transform Understanding the centering of the 2D frequency spectrum from the periodicity of the 2D Fourier transform Fourier Series A Fourier series is a mathematical tool that represents a periodic function as an infinite sum of sines and cosines.\nTrigonometric Series Expansion The Fourier transform was originally based on the trigonometric series expansion: a periodic function can be expressed as a linear combination of sine and cosine functions:\n$$ f(t) = a_0 + \\sum_{n=1}^{\\infty} a_n \\cos(n\\omega_0 t) + b_n \\sin(n\\omega_0 t) $$\nwhere $\\omega_0 = \\frac{2\\pi}{T}$ is the fundamental frequency.\nComplex Exponential Form Using Euler’s formulas:\n$$ \\cos(x) = \\frac{e^{ix} + e^{-ix}}{2}, \\quad \\sin(x) = \\frac{e^{ix} - e^{-ix}}{2i} $$\nthe Fourier series can be rewritten in the complex exponential form:\n$$ f(t) = \\sum_{n=-\\infty}^{\\infty} c_n e^{in\\omega_0 t} $$\nwhere $c_n$ are complex coefficients that encapsulate both sine and cosine information. The Euler form is more symmetric and easier to manipulate, and it lays the foundation for generalizing to continuous and multidimensional cases.\nHere, the index $n$ runs from $-\\infty$ to $\\infty$.\nDiscrete Fourier Transform (DFT) The Discrete Fourier Transform is a crucial branch of Fourier analysis. It converts a finite-length discrete signal (typically digital) from the time or spatial domain into the frequency domain, representing it as a sum of complex frequency components.\nIn practice, we work with finite, discrete data. The DFT is defined as:\n$$ X[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-i \\frac{2\\pi}{N}kn}, \\quad k = 0, 1, \\dots, N-1 $$\nThe corresponding inverse transform is:\n$$ x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] \\cdot e^{i \\frac{2\\pi}{N}kn} $$\nThe DFT maps the time-domain signal $x[n]$ to the frequency-domain $X[k]$, where each $X[k]$ corresponds to the amplitude and phase of a specific frequency component.\nNegative Frequencies and Periodic Reordering In the DFT, negative frequencies naturally arise.\nAlthough the DFT index $k$ is a non-negative integer, the Fourier frequencies exhibit periodicity modulo $N$:\n$$ e^{-i \\frac{2\\pi}{N} k n} = e^{-i \\frac{2\\pi}{N} (k + mN) n}, \\quad \\forall m \\in \\mathbb{Z} $$\nThus, frequency $k = N - 1$ is equivalent to $-1$, $k = N - 2$ to $-2$, and so on.\nWe can reinterpret the frequency indices to span from negative to positive frequencies:\n$$ k = -\\frac{N}{2}, \\dots, -1, 0, 1, \\dots, \\frac{N}{2} - 1 \\quad (\\text{for even } N) $$\nBy reordering (or centering) the spectrum, the result aligns more intuitively with symmetric positive and negative frequency components.\nAfter applying a 2D Fourier transform to an image, we often center the spectrum so that low-frequency components appear at the center.\n2D Fourier Transform The Fourier transform can be extended from the 1D case to 2D.\nWhat Is the 2D Fourier Transform? The continuous 2D Fourier transform of a function $f(x, y)$ is defined as:\n$$ F(u, v) = \\iint_{-\\infty}^{\\infty} f(x, y) \\cdot e^{-i2\\pi (ux + vy)} , dx,dy $$\nIts inverse is:\n$$ f(x, y) = \\iint_{-\\infty}^{\\infty} F(u, v) \\cdot e^{i2\\pi (ux + vy)} , du,dv $$\nHere, $(x, y)$ are spatial-domain coordinates and $(u, v)$ are frequency-domain coordinates. The transform $F(u, v)$ describes the amplitude and phase of each frequency component in the signal.\n2D Discrete Fourier Transform For a discrete $M \\times N$ function $f[m, n]$, the 2D DFT is defined as:\n$$ F[k, l] = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} f[m, n] \\cdot e^{-i 2\\pi \\left( \\frac{km}{M} + \\frac{ln}{N} \\right)} $$\nThe inverse 2D DFT is:\n$$ f[m, n] = \\frac{1}{MN} \\sum_{k=0}^{M-1} \\sum_{l=0}^{N-1} F[k, l] \\cdot e^{i 2\\pi \\left( \\frac{km}{M} + \\frac{ln}{N} \\right)} $$\nPeriodicity of the 2D Fourier Transform Like the 1D DFT, the 2D DFT is periodic:\n$$ F[k + M, l] = F[k, l], \\quad F[k, l + N] = F[k, l] $$\nThis means the spectrum repeats in both horizontal and vertical directions.\nThe frequency indices $k$ and $l$ range from $0$ to $M-1$ and $0$ to $N-1$, but these indices are understood modulo the respective dimensions. For example, when $k \u0026gt; \\tfrac{M}{2}$, the actual frequency corresponds to the negative frequency $k - M$. The same applies for $l \u0026gt; \\tfrac{N}{2}$.\nHow to Perform a 2D Fourier Transform on an Image In Python, libraries like NumPy or OpenCV make it easy to compute a 2D Fourier transform. For example, using NumPy:\nimport numpy as np import matplotlib.pyplot as plt from PIL import Image # Load and convert the image to grayscale img = Image.open(\u0026#39;doge.jpg\u0026#39;).convert(\u0026#39;L\u0026#39;) f = np.array(img) # Compute the 2D Fourier transform F = np.fft.fft2(f) # Magnitude and phase spectra magnitude_spectrum = np.abs(F) phase_spectrum = np.angle(F) log_magnitude = np.log(1 + magnitude_spectrum) # Create subplots fig, axs = plt.subplots(1, 3, figsize=(15, 5)) # Original image axs[0].imshow(f, cmap=\u0026#39;gray\u0026#39;) axs[0].set_title(\u0026#39;Original Image\u0026#39;) axs[0].axis(\u0026#39;off\u0026#39;) # Magnitude spectrum axs[1].imshow(log_magnitude, cmap=\u0026#39;gray\u0026#39;) axs[1].set_title(\u0026#39;Magnitude Spectrum (log)\u0026#39;) axs[1].axis(\u0026#39;off\u0026#39;) # Phase spectrum im = axs[2].imshow(phase_spectrum, cmap=\u0026#39;gray\u0026#39;) axs[2].set_title(\u0026#39;Phase Spectrum\u0026#39;) axs[2].axis(\u0026#39;off\u0026#39;) # Add a colorbar to the phase spectrum fig.colorbar(im, ax=axs[2], shrink=0.7) plt.tight_layout() plt.show() After running this, you will see the magnitude and phase spectra:\nInverse Transform to Recover the Image (Magnitude + Phase) The Fourier transform results are complex, containing magnitude and phase information. Retaining only one of them is insufficient to reconstruct the original image.\nTo recover the image:\nimport numpy as np import matplotlib.pyplot as plt from PIL import Image # Load and convert the image to grayscale img = Image.open(\u0026#39;doge.jpg\u0026#39;).convert(\u0026#39;L\u0026#39;) f = np.array(img) # Compute the 2D Fourier transform F = np.fft.fft2(f) # Perform the inverse Fourier transform recovered = np.fft.ifft2(F) recovered_real = np.real(recovered) plt.imshow(recovered_real, cmap=\u0026#39;gray\u0026#39;) plt.title(\u0026#39;Recovered Image from IFFT\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() The reconstructed image should match the original.\nCentering the Spectrum To observe the frequency distribution more clearly, we often center the spectrum so that low frequencies are in the middle and high frequencies are around the edges.\nYou can use np.fft.fftshift to center the spectrum:\nimport cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread(\u0026#39;doge.jpg\u0026#39;, cv2.IMREAD_GRAYSCALE) f = np.fft.fft2(img) fshift = np.fft.fftshift(f) magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\u0026#39;Original Image\u0026#39;) plt.imshow(img, cmap=\u0026#39;gray\u0026#39;) plt.subplot(1, 2, 2) plt.title(\u0026#39;Magnitude Spectrum\u0026#39;) plt.imshow(magnitude_spectrum, cmap=\u0026#39;gray\u0026#39;) plt.show() After centering, the low-frequency components move to the center, making it easier to observe textures and directional patterns.\nHigh-level contours are represented by the low-frequency center, while fine details and textures are captured by the high-frequency components at the edges.\n","permalink":"https://cronrpc.github.io/en/posts/2d-fourier-transform/","summary":"\u003cp\u003eThis article focuses solely on the following topics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTransition from trigonometric series expansion to the Euler form of the Fourier transform\u003c/li\u003e\n\u003cli\u003eWhat the Discrete Fourier Transform is\u003c/li\u003e\n\u003cli\u003eWhat the 2D Fourier Transform is\u003c/li\u003e\n\u003cli\u003eExplanation of the 2D Discrete Fourier Transform from the 2D Fourier Transform\u003c/li\u003e\n\u003cli\u003eHow to perform a 2D Fourier transform on an image\u003c/li\u003e\n\u003cli\u003ePeriodicity issues in the 2D Fourier transform\u003c/li\u003e\n\u003cli\u003eUnderstanding the centering of the 2D frequency spectrum from the periodicity of the 2D Fourier transform\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"fourier-series\"\u003eFourier Series\u003c/h2\u003e\n\u003cp\u003eA Fourier series is a mathematical tool that represents a periodic function as an infinite sum of sines and cosines.\u003c/p\u003e","title":"2D Fourier Transform"}]